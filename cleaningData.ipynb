{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179d1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651755db",
   "metadata": {},
   "source": [
    "# Étape 1 : Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0150ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sep=\",\", encoding=\"ISO-8859-1\", header=None, rename_columns=None):\n",
    "    \"\"\"Charge les données depuis un fichier CSV.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, sep=sep, encoding=encoding, header=header)\n",
    "        # Apply column renaming during loading\n",
    "        if rename_columns:\n",
    "            data.columns = rename_columns\n",
    "            print(f\"Colonnes renommées : {rename_columns}\")\n",
    "        print(f\"Données chargées avec succès depuis '{file_path}'.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f31f6",
   "metadata": {},
   "source": [
    "# Étape 2 : Sélection des colonnes à conserver \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15644a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_necessary_columns(data, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Garde uniquement les colonnes spécifiées dans le DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        initial_columns = data.columns.tolist()\n",
    "        data = data[columns_to_keep]\n",
    "        print(f\"\\nColonnes conservées : {columns_to_keep}\")\n",
    "        dropped_columns = [col for col in initial_columns if col not in columns_to_keep]\n",
    "        if dropped_columns:\n",
    "            print(f\"Colonnes supprimées : {dropped_columns}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sélection des colonnes : {e}\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cd0da",
   "metadata": {},
   "source": [
    "# Étape 3 : Nettoyage des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83db36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, rules):\n",
    "    \"\"\"\n",
    "    Effectue le nettoyage des données selon les règles spécifiques.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_columns = rules.get(\"columns_to_keep\", [])\n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Erreur : Les colonnes suivantes sont manquantes dans les données : {missing_columns}\")\n",
    "        return data  # Return the data without cleaning\n",
    "\n",
    "    # Renommage des colonnes si nécessaire\n",
    "    if \"rename_columns\" in rules:\n",
    "        data.columns = rules[\"rename_columns\"]\n",
    "        print(\"Colonnes renommées.\")\n",
    "\n",
    "    # Suppression des doublons\n",
    "    data = data.drop_duplicates()\n",
    "    print(f\"Doublons supprimés. Nouvelle taille : {data.shape}\")\n",
    "\n",
    "    # Remplacement des valeurs manquantes\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in rules.get(\"numeric_fill\", {}):\n",
    "            data[col].fillna(rules[\"numeric_fill\"][col], inplace=True)\n",
    "        else:\n",
    "            data[col].fillna(0, inplace=True)  # Valeur par défaut\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].str.strip()  # Strip spaces\n",
    "        data[col].replace(\"\", \"-\", inplace=True)  # Replace empty strings with '-'\n",
    "        data[col].fillna(\"Non spécifié\", inplace=True)\n",
    "\n",
    "    # Custom cleaning logic for Depot\n",
    "    if \"custom_cleaning_steps\" in rules:\n",
    "        for step in rules[\"custom_cleaning_steps\"]:\n",
    "            if step == \"handle_currency_and_country\":\n",
    "                def update_devise(row):\n",
    "                    if pd.isna(row['GDE_PAYS']) or str(row['GDE_PAYS']).strip() in ['', '-']:\n",
    "                        return row['GDE_DEVISE']\n",
    "                    elif row['GDE_PAYS'] == 'CIV': return 'XOF'\n",
    "                    elif row['GDE_PAYS'] == 'TUN': return 'TND'\n",
    "                    return row['GDE_DEVISE']\n",
    "                \n",
    "                data['GDE_DEVISE'] = data.apply(update_devise, axis=1)\n",
    "                \n",
    "                def fill_country(row):\n",
    "                    if row['GDE_PAYS'] in ['-', 'nan'] and row['GDE_DEVISE'] == 'TND':\n",
    "                        return 'TUN'\n",
    "                    return row['GDE_PAYS']\n",
    "                \n",
    "                data['GDE_PAYS'] = data.apply(fill_country, axis=1)\n",
    "                print(\"Custom currency/country logic applied\")\n",
    "            \n",
    "            elif step == \"clean_missing_values\":\n",
    "                data['GDE_PAYS'] = data['GDE_PAYS'].str.strip()\n",
    "                data.replace(['', 'nan', 'NaN', None], '-', inplace=True)\n",
    "                data.fillna('-', inplace=True)\n",
    "                print(\"Missing values cleaned\")\n",
    "\n",
    "    # Identification et suppression des anomalies\n",
    "    if \"anomalies\" in rules:\n",
    "        try:\n",
    "            anomalies = data.query(rules[\"anomalies\"])\n",
    "            if not anomalies.empty:\n",
    "                print(f\"{len(anomalies)} anomalies détectées.\")\n",
    "                data = data[~data.index.isin(anomalies.index)]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'application des anomalies : {e}\")\n",
    "\n",
    "    print(\"Nettoyage des données terminé.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8eaaf3",
   "metadata": {},
   "source": [
    "# Étape 4 : Transformation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1aac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, rules):\n",
    "    \"\"\"\n",
    "    Transforme les données : encodage, création de nouvelles features, etc.\n",
    "    \"\"\"\n",
    "    # Encodage des variables catégoriques\n",
    "    if \"categorical_columns\" in rules:\n",
    "        for col in rules[\"categorical_columns\"]:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "        print(\"Encodage des variables catégoriques terminé.\")\n",
    "\n",
    "    # Conversion des dates en format datetime\n",
    "    if \"date_columns\" in rules:\n",
    "        for col in rules[\"date_columns\"]:\n",
    "            data[col] = pd.to_datetime(data[col], errors=\"coerce\")\n",
    "\n",
    "    # Création de nouvelles features\n",
    "    if \"new_features\" in rules:\n",
    "        for feature_name, feature_formula in rules[\"new_features\"].items():\n",
    "            data[feature_name] = eval(feature_formula)\n",
    "        print(\"Création de nouvelles features terminée.\")\n",
    "\n",
    "    print(\"Transformation des données terminée.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb45ed4",
   "metadata": {},
   "source": [
    "# Étape 5 : Validation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c2b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data):\n",
    "    \"\"\"\n",
    "    Valide les données pour s'assurer qu'elles sont prêtes pour l'analyse.\n",
    "    \"\"\"\n",
    "    # Vérification des valeurs manquantes\n",
    "    missing_values = data.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Attention : {missing_values} valeurs manquantes restantes.\")\n",
    "    else:\n",
    "        print(\"Aucune valeur manquante dans les données.\")\n",
    "\n",
    "    # Vérification des types de données\n",
    "    print(\"Types de données dans le DataFrame :\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    print(\"Validation des données terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76107f",
   "metadata": {},
   "source": [
    "# Pipeline principal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5fb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, output_file, rules):\n",
    "    \"\"\"\n",
    "    Prétraite les données selon les règles spécifiques.\n",
    "    \"\"\"\n",
    "    # Étape 1 : Chargement des données\n",
    "    data = load_data(file_path, **rules.get(\"load_params\", {}))\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    # Étape 2 : Sélection des colonnes à conserver\n",
    "    if \"columns_to_keep\" in rules:\n",
    "        data = keep_only_necessary_columns(data, rules[\"columns_to_keep\"])\n",
    "\n",
    "    # Étape 3 : Nettoyage des données\n",
    "    data = clean_data(data, rules)\n",
    "\n",
    "    # Étape 4 : Transformation des données\n",
    "    data = transform_data(data, rules)\n",
    "\n",
    "    # Étape 5 : Validation des données\n",
    "    validate_data(data)\n",
    "\n",
    "    # Sauvegarde des données prétraitées\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Données prétraitées sauvegardées dans '{output_file}'.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f92d7",
   "metadata": {},
   "source": [
    "# Configuration des règles pour chaque fichier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0baac2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = {\n",
    "    \"Article\": {\n",
    "        \"load_params\": {\"header\": 0},\n",
    "        \"columns_to_keep\": [\"ID_Article\", \"Nom_Article\", \"Prix_vente_TTC\", \"Prix_achat_HT\", \"Categorie\", \"Quantite_Stock\", \"Date_Creation\"],\n",
    "        \"numeric_fill\": {\"Prix_vente_TTC\": 0, \"Prix_achat_HT\": 0},\n",
    "        \"anomalies\": \"(Prix_vente_TTC < 0) | (Prix_achat_HT < 0)\",\n",
    "        \"categorical_columns\": [\"Categorie\"],\n",
    "        \"date_columns\": [\"Date_Creation\"],\n",
    "        \"new_features\": {\n",
    "            \"Marge_Brute\": \"data['Prix_vente_TTC'] - data['Prix_achat_HT']\",\n",
    "            \"Taux_Marge\": \"data['Marge_Brute'] / data['Prix_achat_HT'].replace(0, np.nan)\"\n",
    "        }\n",
    "    },\n",
    "    \"Dispos\": {\n",
    "        \"load_params\": {\"header\": 0},\n",
    "        \"columns_to_keep\": [\"GQ_STOCKMIN\", \"GQ_STOCKMAX\", \"GQ_PHYSIQUE\", \"GQ_DATECREATION\"],\n",
    "        \"numeric_fill\": {\"GQ_STOCKMIN\": 0, \"GQ_STOCKMAX\": \"data['GQ_PHYSIQUE'] * 2\"},\n",
    "        \"anomalies\": \"(GQ_PHYSIQUE > GQ_STOCKMAX) | (GQ_PHYSIQUE < GQ_STOCKMIN)\",\n",
    "        \"date_columns\": [\"GQ_DATECREATION\"],\n",
    "        \"new_features\": {\n",
    "            \"Taux_Remplissage\": \"data['GQ_PHYSIQUE'] / data['GQ_STOCKMAX'].replace(0, np.nan)\",\n",
    "            \"Stock_Disponible\": \"data['GQ_PHYSIQUE'] - data['GQ_STOCKMIN']\"\n",
    "        }\n",
    "    },\n",
    "    \"Tiers\": {\n",
    "        \"load_params\": {\"header\": None},\n",
    "        \"columns_to_keep\": [\"T_AUXILIAIRE\", \"T_TIERS\", \"T_DEVISE\", \"T_NATURE\", \"T_REMISE\", \"T_VILLE\", \"T_PAYS\", \"T_DATECREATION\", \"T_DATEFERMETURE\"],\n",
    "        \"numeric_fill\": {},\n",
    "        \"anomalies\": \"(T_DATECREATION > T_DATEFERMETURE) & (T_DATEFERMETURE != '1900-01-01 00:00:00')\",\n",
    "        \"categorical_columns\": [\"T_DEVISE\", \"T_NATURE\", \"T_VILLE\", \"T_PAYS\"],\n",
    "        \"date_columns\": [\"T_DATECREATION\", \"T_DATEFERMETURE\"],\n",
    "        \"new_features\": {\n",
    "            \"Anciennete\": \"(pd.Timestamp.now() - data['T_DATECREATION']).dt.days / 365\",\n",
    "            \"Statut_Actif\": \"np.where(data['T_DATEFERMETURE'] == '1900-01-01 00:00:00', 1, 0)\"\n",
    "        }\n",
    "    },\n",
    "    \"Depot\": {\n",
    "        \"load_params\": {\n",
    "            \"header\": None,\n",
    "            \"rename_columns\": [\"GDE_DEPOT\", \"GDE_VILLE\", \"GDE_PAYS\", \"GDE_DEVISE\"]\n",
    "        },\n",
    "        \"columns_to_keep\": [\"GDE_DEPOT\", \"GDE_VILLE\", \"GDE_PAYS\", \"GDE_DEVISE\"],\n",
    "        \"numeric_fill\": {},\n",
    "        \"anomalies\": \"(GDE_PAYS == '-') | (GDE_DEVISE == '-')\",\n",
    "        \"categorical_columns\": [\"GDE_PAYS\", \"GDE_DEVISE\"],\n",
    "        \"custom_cleaning\": True\n",
    "    },\n",
    "    \"Ligne\": {\n",
    "        \"load_params\": {\n",
    "        \"header\": None,\n",
    "        \"rename_columns\": [\n",
    "            \"GL_NUMERO\", \"GL_NUMLIGNE\", \"GL_ARTICLE\", \"GL_TIERS\", \"GL_DEPOT\",\n",
    "            \"GL_PUHT\", \"GL_PUTTC\", \"GL_PRHT\", \"GL_QTEFACT\", \"GL_REMISELIGNE\"\n",
    "        ]\n",
    "    },\n",
    "    \"columns_to_keep\": [\n",
    "        \"GL_NUMERO\", \"GL_NUMLIGNE\", \"GL_ARTICLE\", \"GL_TIERS\", \"GL_DEPOT\",\n",
    "        \"GL_PUHT\", \"GL_PUTTC\", \"GL_PRHT\", \"GL_QTEFACT\", \"GL_REMISELIGNE\"\n",
    "    ],\n",
    "    \"numeric_fill\": {\n",
    "        \"GL_PUHT\": 0, \"GL_PUTTC\": 0, \"GL_PRHT\": 0, \"GL_QTEFACT\": 0, \"GL_REMISELIGNE\": 0\n",
    "    },\n",
    "    \"anomalies\": \"(GL_PUHT > GL_PUTTC) | (GL_REMISELIGNE < 0) | (GL_REMISELIGNE > 100) | (GL_QTEFACT < 0)\",\n",
    "    \"categorical_columns\": [],\n",
    "    \"new_features\": {}\n",
    "},\n",
    "    \"Piece\": {\n",
    "        \"load_params\": {\n",
    "            \"header\": None,\n",
    "            \"rename_columns\": [\n",
    "                \"GP_NATUREPIECEG\", \"GP_SOUCHE\", \"GP_NUMERO\", \"GP_TOTALHT\", \n",
    "                \"GP_TOTALTTC\", \"GP_DATEPIECE\", \"GP_DEPOT\", \"GP_TIERS\", \"GP_DEVISE\"\n",
    "            ]\n",
    "        },\n",
    "        \"columns_to_keep\": [\"GP_NATUREPIECEG\", \"GP_SOUCHE\", \"GP_NUMERO\", \"GP_TOTALHT\", \"GP_TOTALTTC\", \"GP_DATEPIECE\", \"GP_DEPOT\", \"GP_TIERS\", \"GP_DEVISE\"],\n",
    "        \"numeric_fill\": {\"GP_TOTALHT\": 0, \"GP_TOTALTTC\": 0},\n",
    "        \"anomalies\": \"(GP_TOTALHT > GP_TOTALTTC) | (GP_TOTALHT < 0) | (GP_TOTALTTC < 0) | (GP_DATEPIECE.dt.year < 2023)\",\n",
    "        \"date_columns\": [\"GP_DATEPIECE\"],\n",
    "        \"categorical_columns\": [\"GP_DEVISE\"],\n",
    "        \"new_features\": {}\n",
    "    },\n",
    "    \"Remise\": {\n",
    "        \"load_params\": {\n",
    "            \"header\": None,\n",
    "            \"rename_columns\": [\n",
    "                \"MLR_ETABLISSEMENT\", \"MLR_SOUCHE\", \"MLR_NUMERO\", \"MLR_NUMLIGNE\",\n",
    "                \"MLR_ORGREMISE\", \"MLR_TYPEREMISE\", \"MLR_REMISE\", \"MLR_VALEURREMDEV\",\n",
    "                \"MLR_MONTANTHTDEV\", \"MLR_MONTANTTTCDEV\"\n",
    "            ]\n",
    "        },\n",
    "        \"columns_to_keep\": [\"MLR_ETABLISSEMENT\",\"MLR_SOUCHE\",\"MLR_NUMERO\",\"MLR_NUMLIGNE\",\"MLR_ORGREMISE\",\"MLR_TYPEREMISE\",\"MLR_REMISE\",\"MLR_VALEURREMDEV\",\"MLR_MONTANTHTDEV\",\"MLR_MONTANTTTCDEV\"],\n",
    "        \"numeric_fill\": {\"MLR_REMISE\":0,\"MLR_VALEURREMDEV\":0,\"MLR_MONTANTHTDEV\":0,\"MLR_MONTANTTTCDEV\":0},\n",
    "        \"anomalies\": \"(MLR_MONTANTHTDEV > MLR_MONTANTTTCDEV) | (MLR_REMISE <= 0) | (MLR_MONTANTTTCDEV < 0)\",\n",
    "        \"categorical_columns\": [],\n",
    "        \"new_features\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93317f69",
   "metadata": {},
   "source": [
    "#  Exécution principale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8e7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Traitement du fichier : Article.csv ===\n",
      "Erreur lors du chargement des données : [Errno 2] No such file or directory: 'Article.csv'\n",
      "\n",
      "=== Traitement du fichier : DISPOS.csv ===\n",
      "Erreur lors du chargement des données : [Errno 2] No such file or directory: 'DISPOS.csv'\n",
      "\n",
      "=== Traitement du fichier : Tier.csv ===\n",
      "Erreur lors du chargement des données : [Errno 2] No such file or directory: 'Tier.csv'\n",
      "\n",
      "=== Traitement du fichier : Depot.csv ===\n",
      "Colonnes renommées : ['GDE_DEPOT', 'GDE_VILLE', 'GDE_PAYS', 'GDE_DEVISE']\n",
      "Données chargées avec succès depuis 'Depot.csv'.\n",
      "\n",
      "Colonnes conservées : ['GDE_DEPOT', 'GDE_VILLE', 'GDE_PAYS', 'GDE_DEVISE']\n",
      "Doublons supprimés. Nouvelle taille : (67, 4)\n",
      "7 anomalies détectées.\n",
      "Nettoyage des données terminé.\n",
      "Encodage des variables catégoriques terminé.\n",
      "Transformation des données terminée.\n",
      "Aucune valeur manquante dans les données.\n",
      "Types de données dans le DataFrame :\n",
      "GDE_DEPOT     object\n",
      "GDE_VILLE     object\n",
      "GDE_PAYS       int32\n",
      "GDE_DEVISE     int32\n",
      "dtype: object\n",
      "Validation des données terminée.\n",
      "Données prétraitées sauvegardées dans 'Depot_Preprocessed.csv'.\n",
      "\n",
      "=== Traitement du fichier : lastLigne.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].replace(\"\", \"-\", inplace=True)  # Replace empty strings with '-'\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(\"Non spécifié\", inplace=True)\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\2951659249.py:4: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep=sep, encoding=encoding, header=header)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes renommées : ['GL_NUMERO', 'GL_NUMLIGNE', 'GL_ARTICLE', 'GL_TIERS', 'GL_DEPOT', 'GL_PUHT', 'GL_PUTTC', 'GL_PRHT', 'GL_QTEFACT', 'GL_REMISELIGNE']\n",
      "Données chargées avec succès depuis 'lastLigne.csv'.\n",
      "\n",
      "Colonnes conservées : ['GL_NUMERO', 'GL_NUMLIGNE', 'GL_ARTICLE', 'GL_TIERS', 'GL_DEPOT', 'GL_PUHT', 'GL_PUTTC', 'GL_PRHT', 'GL_QTEFACT', 'GL_REMISELIGNE']\n",
      "Doublons supprimés. Nouvelle taille : (10236953, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = data[col].str.strip()  # Strip spaces\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].replace(\"\", \"-\", inplace=True)  # Replace empty strings with '-'\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col].replace(\"\", \"-\", inplace=True)  # Replace empty strings with '-'\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(\"Non spécifié\", inplace=True)\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col].fillna(\"Non spécifié\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'application des anomalies : '<' not supported between instances of 'str' and 'int'\n",
      "Nettoyage des données terminé.\n",
      "Encodage des variables catégoriques terminé.\n",
      "Création de nouvelles features terminée.\n",
      "Transformation des données terminée.\n",
      "Aucune valeur manquante dans les données.\n",
      "Types de données dans le DataFrame :\n",
      "GL_NUMERO         object\n",
      "GL_NUMLIGNE       object\n",
      "GL_ARTICLE        object\n",
      "GL_TIERS          object\n",
      "GL_DEPOT          object\n",
      "GL_PUHT           object\n",
      "GL_PUTTC          object\n",
      "GL_PRHT           object\n",
      "GL_QTEFACT        object\n",
      "GL_REMISELIGNE    object\n",
      "dtype: object\n",
      "Validation des données terminée.\n",
      "Données prétraitées sauvegardées dans 'Ligne_Preprocessed.csv'.\n",
      "\n",
      "=== Traitement du fichier : Piece.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\2951659249.py:4: DtypeWarning: Columns (0,1,3,4,5,6,7,9,10,11,12,13,14,15,17,18,22,23,24,25,26,27,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep=sep, encoding=encoding, header=header)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des données : Length mismatch: Expected axis has 34 elements, new values have 9 elements\n",
      "\n",
      "=== Traitement du fichier : Remise.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\2951659249.py:4: DtypeWarning: Columns (0,1,2,3,4,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep=sep, encoding=encoding, header=header)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes renommées : ['MLR_ETABLISSEMENT', 'MLR_SOUCHE', 'MLR_NUMERO', 'MLR_NUMLIGNE', 'MLR_ORGREMISE', 'MLR_TYPEREMISE', 'MLR_REMISE', 'MLR_VALEURREMDEV', 'MLR_MONTANTHTDEV', 'MLR_MONTANTTTCDEV']\n",
      "Données chargées avec succès depuis 'Remise.csv'.\n",
      "\n",
      "Colonnes conservées : ['MLR_ETABLISSEMENT', 'MLR_SOUCHE', 'MLR_NUMERO', 'MLR_NUMLIGNE', 'MLR_ORGREMISE', 'MLR_TYPEREMISE', 'MLR_REMISE', 'MLR_VALEURREMDEV', 'MLR_MONTANTHTDEV', 'MLR_MONTANTTTCDEV']\n",
      "Doublons supprimés. Nouvelle taille : (3016669, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].replace(\"\", \"-\", inplace=True)  # Replace empty strings with '-'\n",
      "C:\\Users\\azizm\\AppData\\Local\\Temp\\ipykernel_19452\\1173233911.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(\"Non spécifié\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'application des anomalies : '<=' not supported between instances of 'str' and 'int'\n",
      "Nettoyage des données terminé.\n",
      "Encodage des variables catégoriques terminé.\n",
      "Création de nouvelles features terminée.\n",
      "Transformation des données terminée.\n",
      "Aucune valeur manquante dans les données.\n",
      "Types de données dans le DataFrame :\n",
      "MLR_ETABLISSEMENT    object\n",
      "MLR_SOUCHE           object\n",
      "MLR_NUMERO           object\n",
      "MLR_NUMLIGNE         object\n",
      "MLR_ORGREMISE        object\n",
      "MLR_TYPEREMISE       object\n",
      "MLR_REMISE           object\n",
      "MLR_VALEURREMDEV     object\n",
      "MLR_MONTANTHTDEV     object\n",
      "MLR_MONTANTTTCDEV    object\n",
      "dtype: object\n",
      "Validation des données terminée.\n",
      "Données prétraitées sauvegardées dans 'Remise_Preprocessed.csv'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Liste des fichiers à prétraiter\n",
    "    files_to_process = [\n",
    "        {\"file_path\": \"Article.csv\", \"output_file\": \"Article_Preprocessed.csv\", \"rules\": RULES[\"Article\"]},\n",
    "        {\"file_path\": \"DISPOS.csv\", \"output_file\": \"DISPO_Preprocessed.csv\", \"rules\": RULES[\"Dispos\"]},\n",
    "        {\"file_path\": \"Tier.csv\", \"output_file\": \"TIERS_Preprocessed.csv\", \"rules\": RULES[\"Tiers\"]},\n",
    "        {\"file_path\": \"Depot.csv\", \"output_file\": \"Depot_Preprocessed.csv\", \"rules\": RULES[\"Depot\"]},\n",
    "        {\"file_path\": \"lastLigne.csv\", \"output_file\": \"Ligne_Preprocessed.csv\", \"rules\": RULES[\"Ligne\"]},\n",
    "        {\"file_path\": \"Piece.csv\", \"output_file\": \"Piece_Preprocessed.csv\", \"rules\": RULES[\"Piece\"]},\n",
    "        {\"file_path\": \"Remise.csv\", \"output_file\": \"Remise_Preprocessed.csv\", \"rules\": RULES[\"Remise\"]}\n",
    "    ]\n",
    "\n",
    "    # Prétraitement de chaque fichier\n",
    "    for file_info in files_to_process:\n",
    "        print(f\"\\n=== Traitement du fichier : {file_info['file_path']} ===\")\n",
    "        preprocess_data(\n",
    "            file_path=file_info[\"file_path\"],\n",
    "            output_file=file_info[\"output_file\"],\n",
    "            rules=file_info[\"rules\"]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
