{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179d1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651755db",
   "metadata": {},
   "source": [
    "# Étape 1 : Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0150ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sep=\",\", encoding=\"utf-8\", header=None):\n",
    "    \"\"\"\n",
    "    Charge les données depuis un fichier CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, sep=sep, encoding=encoding, header=header)\n",
    "        print(f\"Données chargées avec succès depuis '{file_path}'.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f31f6",
   "metadata": {},
   "source": [
    "# Étape 2 : Sélection des colonnes à conserver \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15644a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_necessary_columns(data, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Garde uniquement les colonnes spécifiées dans le DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        initial_columns = data.columns.tolist()\n",
    "        data = data[columns_to_keep]\n",
    "        print(f\"\\nColonnes conservées : {columns_to_keep}\")\n",
    "        dropped_columns = [col for col in initial_columns if col not in columns_to_keep]\n",
    "        if dropped_columns:\n",
    "            print(f\"Colonnes supprimées : {dropped_columns}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sélection des colonnes : {e}\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cd0da",
   "metadata": {},
   "source": [
    "# Étape 3 : Nettoyage des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, rules):\n",
    "    \"\"\"\n",
    "    Effectue le nettoyage des données selon les règles spécifiques.\n",
    "    \"\"\"\n",
    "    # Renommage des colonnes si nécessaire\n",
    "    if \"rename_columns\" in rules:\n",
    "        data.columns = rules[\"rename_columns\"]\n",
    "        print(\"Colonnes renommées.\")\n",
    "\n",
    "    # Suppression des doublons\n",
    "    data = data.drop_duplicates()\n",
    "    print(f\"Doublons supprimés. Nouvelle taille : {data.shape}\")\n",
    "\n",
    "    # Remplacement des valeurs manquantes\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in rules.get(\"numeric_fill\", {}):\n",
    "            data[col].fillna(rules[\"numeric_fill\"][col], inplace=True)\n",
    "        else:\n",
    "            data[col].fillna(0, inplace=True)  # Valeur par défaut\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        data[col].fillna(\"Non spécifié\", inplace=True)\n",
    "\n",
    "    # Identification et suppression des anomalies\n",
    "    if \"anomalies\" in rules:\n",
    "        anomalies = data.query(rules[\"anomalies\"])\n",
    "        if not anomalies.empty:\n",
    "            print(f\"{len(anomalies)} anomalies détectées.\")\n",
    "            data = data[~data.index.isin(anomalies.index)]\n",
    "\n",
    "    print(\"Nettoyage des données terminé.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8eaaf3",
   "metadata": {},
   "source": [
    "# Étape 4 : Transformation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1aac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, rules):\n",
    "    \"\"\"\n",
    "    Transforme les données : encodage, création de nouvelles features, etc.\n",
    "    \"\"\"\n",
    "    # Encodage des variables catégoriques\n",
    "    if \"categorical_columns\" in rules:\n",
    "        for col in rules[\"categorical_columns\"]:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "        print(\"Encodage des variables catégoriques terminé.\")\n",
    "\n",
    "    # Conversion des dates en format datetime\n",
    "    if \"date_columns\" in rules:\n",
    "        for col in rules[\"date_columns\"]:\n",
    "            data[col] = pd.to_datetime(data[col], errors=\"coerce\")\n",
    "\n",
    "    # Création de nouvelles features\n",
    "    if \"new_features\" in rules:\n",
    "        for feature_name, feature_formula in rules[\"new_features\"].items():\n",
    "            data[feature_name] = eval(feature_formula)\n",
    "        print(\"Création de nouvelles features terminée.\")\n",
    "\n",
    "    print(\"Transformation des données terminée.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb45ed4",
   "metadata": {},
   "source": [
    "# Étape 5 : Validation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data):\n",
    "    \"\"\"\n",
    "    Valide les données pour s'assurer qu'elles sont prêtes pour l'analyse.\n",
    "    \"\"\"\n",
    "    # Vérification des valeurs manquantes\n",
    "    missing_values = data.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Attention : {missing_values} valeurs manquantes restantes.\")\n",
    "    else:\n",
    "        print(\"Aucune valeur manquante dans les données.\")\n",
    "\n",
    "    # Vérification des types de données\n",
    "    print(\"Types de données dans le DataFrame :\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    print(\"Validation des données terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76107f",
   "metadata": {},
   "source": [
    "# Pipeline principal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, output_file, rules):\n",
    "    \"\"\"\n",
    "    Prétraite les données selon les règles spécifiques.\n",
    "    \"\"\"\n",
    "    # Étape 1 : Chargement des données\n",
    "    data = load_data(file_path, **rules.get(\"load_params\", {}))\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    # Étape 2 : Sélection des colonnes à conserver\n",
    "    if \"columns_to_keep\" in rules:\n",
    "        data = keep_only_necessary_columns(data, rules[\"columns_to_keep\"])\n",
    "\n",
    "    # Étape 3 : Nettoyage des données\n",
    "    data = clean_data(data, rules)\n",
    "\n",
    "    # Étape 4 : Transformation des données\n",
    "    data = transform_data(data, rules)\n",
    "\n",
    "    # Étape 5 : Validation des données\n",
    "    validate_data(data)\n",
    "\n",
    "    # Sauvegarde des données prétraitées\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Données prétraitées sauvegardées dans '{output_file}'.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f92d7",
   "metadata": {},
   "source": [
    "# Configuration des règles pour chaque fichier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baac2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = {\n",
    "    \"Article\": {\n",
    "        \"load_params\": {\"header\": 0},\n",
    "        \"columns_to_keep\": [\"ID_Article\", \"Nom_Article\", \"Prix_vente_TTC\", \"Prix_achat_HT\",\n",
    "                            \"Categorie\", \"Quantite_Stock\", \"Date_Creation\"],\n",
    "        \"numeric_fill\": {\"Prix_vente_TTC\": 0, \"Prix_achat_HT\": 0},\n",
    "        \"anomalies\": \"(Prix_vente_TTC < 0) | (Prix_achat_HT < 0)\",\n",
    "        \"categorical_columns\": [\"Categorie\"],\n",
    "        \"date_columns\": [\"Date_Creation\"],\n",
    "        \"new_features\": {\n",
    "            \"Marge_Brute\": \"data['Prix_vente_TTC'] - data['Prix_achat_HT']\",\n",
    "            \"Taux_Marge\": \"data['Marge_Brute'] / data['Prix_achat_HT'].replace(0, np.nan)\"\n",
    "        }\n",
    "    },\n",
    "    \"Dispos\": {\n",
    "        \"load_params\": {\"header\": 0},\n",
    "        \"columns_to_keep\": [\"GQ_STOCKMIN\", \"GQ_STOCKMAX\", \"GQ_PHYSIQUE\", \"GQ_DATECREATION\"],\n",
    "        \"numeric_fill\": {\"GQ_STOCKMIN\": 0, \"GQ_STOCKMAX\": \"data['GQ_PHYSIQUE'] * 2\"},\n",
    "        \"anomalies\": \"(GQ_PHYSIQUE > GQ_STOCKMAX) | (GQ_PHYSIQUE < GQ_STOCKMIN)\",\n",
    "        \"date_columns\": [\"GQ_DATECREATION\"],\n",
    "        \"new_features\": {\n",
    "            \"Taux_Remplissage\": \"data['GQ_PHYSIQUE'] / data['GQ_STOCKMAX'].replace(0, np.nan)\",\n",
    "            \"Stock_Disponible\": \"data['GQ_PHYSIQUE'] - data['GQ_STOCKMIN']\"\n",
    "        }\n",
    "    },\n",
    "    \"Tiers\": {\n",
    "        \"load_params\": {\"header\": None},\n",
    "        \"columns_to_keep\": [\"T_AUXILIAIRE\", \"T_TIERS\", \"T_DEVISE\", \"T_NATURE\", \"T_REMISE\",\n",
    "                            \"T_VILLE\", \"T_PAYS\", \"T_DATECREATION\", \"T_DATEFERMETURE\"],\n",
    "        \"numeric_fill\": {},\n",
    "        \"anomalies\": \"(T_DATECREATION > T_DATEFERMETURE) & (T_DATEFERMETURE != '1900-01-01 00:00:00')\",\n",
    "        \"categorical_columns\": [\"T_DEVISE\", \"T_NATURE\", \"T_VILLE\", \"T_PAYS\"],\n",
    "        \"date_columns\": [\"T_DATECREATION\", \"T_DATEFERMETURE\"],\n",
    "        \"new_features\": {\n",
    "            \"Anciennete\": \"(pd.Timestamp.now() - data['T_DATECREATION']).dt.days / 365\",\n",
    "            \"Statut_Actif\": \"np.where(data['T_DATEFERMETURE'] == '1900-01-01 00:00:00', 1, 0)\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93317f69",
   "metadata": {},
   "source": [
    "#  Exécution principale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Liste des fichiers à prétraiter\n",
    "    files_to_process = [\n",
    "        {\"file_path\": \"Article.csv\", \"output_file\": \"Article_Preprocessed.csv\", \"rules\": RULES[\"Article\"]},\n",
    "        {\"file_path\": \"DISPOS.csv\", \"output_file\": \"DISPO_Preprocessed.csv\", \"rules\": RULES[\"Dispos\"]},\n",
    "        {\"file_path\": \"Tier.csv\", \"output_file\": \"TIERS_Preprocessed.csv\", \"rules\": RULES[\"Tiers\"]}\n",
    "    ]\n",
    "\n",
    "    # Prétraitement de chaque fichier\n",
    "    for file_info in files_to_process:\n",
    "        print(f\"\\n=== Traitement du fichier : {file_info['file_path']} ===\")\n",
    "        preprocess_data(\n",
    "            file_path=file_info[\"file_path\"],\n",
    "            output_file=file_info[\"output_file\"],\n",
    "            rules=file_info[\"rules\"]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
